{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Top Repositories for GitHub Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INDEX : \n",
    "1. Introduction to Web Scraping \n",
    "2. Definition of Problem Statement\n",
    "3. Technologies Used\n",
    "4. Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction to Web Scraping\n",
    "\n",
    "- Web scraping is a technique used to extract data from websites by parsing the HTML content of web pages. It allows users to collect large amounts of data efficiently, which can be used for various purposes such as analysis, research, and automation.\n",
    "- This process typically involves sending a request to a website, retrieving the HTML content, and then extracting specific information using libraries like BeautifulSoup in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Definition of Problem Statement\n",
    "\n",
    "- The goal of this project is to scrape TOP 30 topics' information from GitHub's topics page. \n",
    "- Specifically, the task is to extract the titles, descriptions, and URLs of various topics listed on the page.\n",
    "- This information will then be structured into a Pandas DataFrames and .csv files for further analysis or storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Technologies Used\n",
    "\n",
    "- Python: The primary programming language used for scripting and automation.\n",
    "- BeautifulSoup: A Python library used for parsing HTML and XML documents, allowing easy navigation and extraction of data.\n",
    "- Requests: A Python library used to send HTTP requests to websites and retrieve HTML content.\n",
    "- Pandas: A data manipulation and analysis library used to organize and structure the scraped data into a DataFrame for easier handling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick a website and describe your objective\n",
    "\n",
    "- Browse through different sites and pick on to scrape. Check the \"Project Ideas\" section for inspiration.\n",
    "- Identify the information you'd like to scrape from the site. Decide the format of the output CSV file.\n",
    "- Summarize your project idea and outline your strategy in a Juptyer notebook. Use the \"New\" button above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline/Strategy : \n",
    "- We're going to scrape - ``https://github.com/topics``\n",
    "-  we'll get a list of the topics, for each topic, we will get topic title, topic page URL and topic description\n",
    "-  For each topic, we will get top 25 repositories in the topic from the topics page\n",
    "-  For each repository, we will grab repo name, username, stars and repo URL\n",
    "-  For each topic we will create a CSV file in the following format :\n",
    "\n",
    "    ```\n",
    "    Repository name,Username,Stars,Repo URL\n",
    "    infinite-scroll,metafizzy,7400,https://github.com/metafizzy/infinite-scroll\n",
    "    tabulator,olifolkerd,6600,https://github.com/olifolkerd/tabulator\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the requests library to download web pages\n",
    "\n",
    "- Inspect the website's HTML source and identify the right URLs to download.\n",
    "- Download and save web pages locally using the requests library.\n",
    "- Create a function to automate downloading for different topics/search queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install requests --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_url = \"https://github.com/topics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(topics_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.status_code\n",
    "# request code 200-209 means response successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_contents = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('webpage.html', 'w', encoding='utf-8') as f:\n",
    "    f.write(page_contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Beautiful Soup to parse and extract information\n",
    "\n",
    "- Parse and explore the structure of downloaded web pages using Beautiful soup.\n",
    "- Use the right properties and methods to extract the required information.\n",
    "- Create functions to extract from the page into lists and dictionaries.\n",
    "(Optional) Use a REST API to acquire additional information if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = BeautifulSoup(page_contents, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_class = \"f3 lh-condensed mb-0 mt-1 Link--primary\"\n",
    "topic_title_tags = doc.findAll('p',{\"class\":selection_class})\n",
    "\n",
    "desc_selector = \"f5 color-fg-muted mb-0 mt-1\"\n",
    "topic_desc_tags = doc.find_all(\"p\",{\"class\" : desc_selector})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_selector = \"f5 color-fg-muted mb-0 mt-1\"\n",
    "topic_desc_tags = doc.find_all(\"p\",{\"class\" : desc_selector})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_titles = []\n",
    "topic_descriptions = []\n",
    "for tag in topic_title_tags:\n",
    "    topic_titles.append(tag.text)\n",
    "for tag in topic_desc_tags:\n",
    "    topic_descriptions.append(tag.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_class = \"f3 lh-condensed mb-0 mt-1 Link--primary\"\n",
    "topic_title_tags = doc.findAll('p',{\"class\":selection_class})\n",
    "\n",
    "desc_selector = \"f5 color-fg-muted mb-0 mt-1\"\n",
    "topic_desc_tags = doc.find_all(\"p\",{\"class\" : desc_selector})\n",
    "\n",
    "topic_link_tags = doc.find_all(\"a\", {\"class\":\"no-underline flex-grow-0\"})\n",
    "\n",
    "topic_titles = []\n",
    "topic_descriptions = []\n",
    "for tag in topic_title_tags:\n",
    "    topic_titles.append(tag.text)\n",
    "for tag in topic_desc_tags:\n",
    "    topic_descriptions.append(tag.text.strip())\n",
    "\n",
    "topic_urls = []\n",
    "base_url = \"https://github.com\"\n",
    "for tag in topic_link_tags:\n",
    "    topic_urls.append(base_url+tag[\"href\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_urls = []\n",
    "base_url = \"https://github.com\"\n",
    "for tag in topic_link_tags:\n",
    "    topic_urls.append(base_url+tag[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_dict = {\n",
    "    \"title\" : topic_titles,\n",
    "    \"description\" : topic_descriptions,\n",
    "    \"url\" : topic_urls\n",
    "}\n",
    "topics_df = pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create CSV file(s) with the extracted information\n",
    "\n",
    "- Create functions for the end-to-end process of downloading, parsing, and saving CSVs.\n",
    "- Execute the function with different inputs to create a dataset of CSV files.\n",
    "- Verify the information in the CSV files by reading them back using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df.to_csv(\"topics.csv\",index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Information out of a topic page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_page_url = topic_urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_page_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(topic_page_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_doc = BeautifulSoup(response.text,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_selection_class = \"f3 color-fg-muted text-normal lh-condensed\"\n",
    "repo_tags = topic_doc.findAll(\"h3\", {\"class\": h3_selection_class})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tags = repo_tags[0].findAll(\"a\")\n",
    "a_tags[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tags[1].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://github.com\"\n",
    "repo_url = base_url + a_tags[1][\"href\"]\n",
    "repo_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_class = \"Counter js-social-count\"\n",
    "star_tags = topic_doc.findAll(\"span\", {\"class\":star_class})\n",
    "len(star_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_tags[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_star_count(stars_str):\n",
    "    stars_str = stars_str.strip()\n",
    "    if stars_str[-1] == \"k\":\n",
    "        return int(float(stars_str[:-1])*1000)\n",
    "    else:\n",
    "       return int(stars_str)\n",
    "\n",
    "parse_star_count(star_tags[0].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(h3_tag, star_tag):\n",
    "    # returns all required info about repository\n",
    "    a_tags = h3_tag.find_all(\"a\")\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url = base_url + a_tags[1][\"href\"]\n",
    "    stars = parse_star_count(star_tag.text.strip())\n",
    "    return username, repo_name, stars, repo_url\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_repo_info(repo_tags[0],star_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_repos_dictionary = {\n",
    "    \"username\":[],\n",
    "    \"repo_name\":[],\n",
    "    \"stars\":[],\n",
    "    \"repo_url\":[]\n",
    "}\n",
    "\n",
    "for i in range(len(repo_tags)):\n",
    "    repo_info = get_repo_info(repo_tags[i],star_tags[i])\n",
    "    topic_repos_dictionary[\"username\"].append(repo_info[0])\n",
    "    topic_repos_dictionary[\"repo_name\"].append(repo_info[1])\n",
    "    topic_repos_dictionary[\"stars\"].append(repo_info[2])\n",
    "    topic_repos_dictionary[\"repo_url\"].append(repo_info[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def get_topic_page(topic_url):\n",
    "    #Download the page\n",
    "    response = requests.get(topic_url)\n",
    "    #Checking the status code\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Failed to load page {} \".format(topic_url))\n",
    "    #parse using beautiful soup\n",
    "    topic_doc = BeautifulSoup(response.text,\"html.parser\")\n",
    "    return topic_doc\n",
    "\n",
    "def get_repo_info(h3_tag, star_tag):\n",
    "    # returns all required info about repository\n",
    "    a_tags = h3_tag.find_all(\"a\")\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url = base_url + a_tags[1][\"href\"]\n",
    "    stars = parse_star_count(star_tag.text.strip())\n",
    "    return username, repo_name, stars, repo_url\n",
    "    \n",
    "\n",
    "def get_topic_repos(topic_doc):\n",
    "    \n",
    "    #Get the h tag containing repo title, URL and username\n",
    "    h3_selection_class = \"f3 color-fg-muted text-normal lh-condensed\"\n",
    "    repo_tags = topic_doc.findAll(\"h3\", {\"class\": h3_selection_class})\n",
    "    #Get star tags\n",
    "    star_class = \"Counter js-social-count\"\n",
    "    star_tags = topic_doc.findAll(\"span\", {\"class\":star_class})\n",
    "\n",
    "    # Dictionary containing All the repos topics\n",
    "    topic_repos_dictionary = {\n",
    "        \"username\":[],\n",
    "        \"repo_name\":[],\n",
    "        \"stars\":[],\n",
    "        \"repo_url\":[]\n",
    "    }\n",
    "    # Get repo Info\n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i],star_tags[i])\n",
    "        topic_repos_dictionary[\"username\"].append(repo_info[0])\n",
    "        topic_repos_dictionary[\"repo_name\"].append(repo_info[1])\n",
    "        topic_repos_dictionary[\"stars\"].append(repo_info[2])\n",
    "        topic_repos_dictionary[\"repo_url\"].append(repo_info[3])\n",
    "    \n",
    "    return pd.DataFrame(topic_repos_dictionary)\n",
    "\n",
    "def scrape_topic(topic_url, topic_name):\n",
    "    fname = topic_name + \".csv\"\n",
    "    if os.path.exists(fname):\n",
    "        print(\"file {} already exists, skipping...\".format(fname))\n",
    "        return\n",
    "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
    "    topic_df.to_csv(fname,index=None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_repos_df = pd.DataFrame(topic_repos_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topic_repos(get_topic_page(topic_urls[4])).to_csv(\"Android.csv\",index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write a single function to :\n",
    "1. Get the list of topics from the topics page of github\n",
    "2. Get the list of top repos from the individual topic pages\n",
    "3. For each topic, create a CSV of the top repos for that topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    selection_class = \"f3 lh-condensed mb-0 mt-1 Link--primary\"\n",
    "    topic_title_tags = doc.findAll('p',{\"class\":selection_class})\n",
    "    topic_titles = []\n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "    return topic_titles\n",
    "\n",
    "def get_topic_description(doc):\n",
    "    desc_selector = \"f5 color-fg-muted mb-0 mt-1\"\n",
    "    topic_desc_tags = doc.find_all(\"p\",{\"class\" : desc_selector})\n",
    "    topic_descriptions = []\n",
    "    for tag in topic_desc_tags:\n",
    "        topic_descriptions.append(tag.text.strip())\n",
    "    return topic_descriptions\n",
    "\n",
    "def get_topic_urls(doc):\n",
    "    topic_link_tags = doc.find_all(\"a\", {\"class\":\"no-underline flex-grow-0\"})\n",
    "    topic_urls = []\n",
    "    base_url = \"https://github.com\"\n",
    "    for tag in topic_link_tags:\n",
    "        topic_urls.append(base_url+tag[\"href\"])\n",
    "    return topic_urls\n",
    "\n",
    "def scrape_topics():\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "            raise Exception(\"Failed to load page {} \".format(topics_url))\n",
    "    topics_dict = {\n",
    "         \"title\": get_topic_titles(doc),\n",
    "         \"description\":get_topic_description(doc),\n",
    "         \"url\":get_topic_urls(doc)\n",
    "    }\n",
    "    return pd.DataFrame(topics_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_repos():\n",
    "    print(\"Scraping list of Topics...\")\n",
    "    topics_df = scrape_topics()\n",
    "\n",
    "    os.makedirs(\"data\",exist_ok=True)\n",
    "    \n",
    "    for index,row in topics_df.iterrows():\n",
    "        print(\"Scraping Top Repository for {}\".format(row[\"title\"]))\n",
    "        #scrape_topic(row[\"url\"],row[\"title\"])\n",
    "        scrape_topic(row[\"url\"],\"data/{}\".format(row[\"title\"]))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing Reference\n",
    "\n",
    "- After working on this project, Iâ€™ve significantly enhanced my Python skills, particularly in web scraping and data extraction using libraries like BeautifulSoup and Requests.\n",
    "- This experience has deepened my understanding of handling HTML content, parsing data, and effectively organizing the extracted information into usable formats.\n",
    "-At last, I would like to extend my gratitude to [Jovian](http://www.youtube.com/@jovianhq) for their In depth Technical Explainations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
